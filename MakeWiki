# Makes programs, downloads sample data, trains a GloVe model, and then evaluates it.
LANGUAGE := yo
DATA_DIR :=  data
RESULTS_DIR :=  results
BUILDDIR := build

DATA_DIR_LANG := $(DATA_DIR)/wiki/$(LANGUAGE)
RESULTS_DIR_LANG :=  $(RESULTS_DIR)/$(LANGUAGE)/

XML_NAME := $(LANGUAGE)wiki-latest-pages-articles.xml.bz2
WIKIURL := https://dumps.wikimedia.org/$(LANGUAGE)wiki/latest/$(XML_NAME)
JSON_NAME := wiki-latest.json.gz

# Data Files
XML_FILE := $(DATA_DIR_LANG)/$(XML_NAME)
JSON_FILE := $(DATA_DIR_LANG)/$(JSON_NAME)
WIKI_PARSED_FILE := $(DATA_DIR_LANG)/parsed.txt

# Glove Files
VOCAB_FILE := $(RESULTS_DIR_LANG)vocab.txt
COOCCURRENCE_FILE := $(RESULTS_DIR_LANG)cooccurrence.bin
COOCCURRENCE_SHUF_FILE := $(RESULTS_DIR_LANG)cooccurrence.shuf.bin
VECTOR_FILE := $(RESULTS_DIR_LANG)vectors
VECTOR_FILE_TXT := $(VECTOR_FILE).txt

# HyperParameters
VERBOSE := 2
MEMORY := 4.0
VOCAB_MIN_COUNT := 5
VECTOR_SIZE := 100
MAX_ITER := 15
WINDOW_SIZE := 15
BINARY := 2
NUM_THREADS := 8
X_MAX := 10

all: build $(VECTOR_FILE_TXT)

get_wiki: $(WIKI_PARSED_FILE)

# evaluate: $(VECTOR_FILE_TXT)
# 	python eval/python/evaluate.py --vocab_file $(VOCAB_FILE) --vectors_file $(VECTOR_FILE_TXT)

$(VECTOR_FILE_TXT): $(COOCCURRENCE_SHUF_FILE)
	$(BUILDDIR)/glove -save-file $(VECTOR_FILE) -threads $(NUM_THREADS) -input-file $(COOCCURRENCE_SHUF_FILE) -x-max $(X_MAX) -iter $(MAX_ITER) -vector-size $(VECTOR_SIZE) -binary $(BINARY) -vocab-file $(VOCAB_FILE) -verbose $(VERBOSE)

$(COOCCURRENCE_SHUF_FILE): $(COOCCURRENCE_FILE)
	$(BUILDDIR)/shuffle -memory $(MEMORY) -verbose $(VERBOSE) < $(COOCCURRENCE_FILE) > $(COOCCURRENCE_SHUF_FILE)

$(COOCCURRENCE_FILE): $(VOCAB_FILE)
	$(BUILDDIR)/cooccur -memory $(MEMORY) -vocab-file $(VOCAB_FILE) -verbose $(VERBOSE) -window-size $(WINDOW_SIZE) < $(WIKI_PARSED_FILE) > $(COOCCURRENCE_FILE)

$(VOCAB_FILE): $(WIKI_PARSED_FILE)
	mkdir -p $(RESULTS_DIR_LANG)
	$(BUILDDIR)/vocab_count -min-count $(VOCAB_MIN_COUNT) -verbose $(VERBOSE) < $(WIKI_PARSED_FILE) > $(VOCAB_FILE)

# Tokenize wikipedia
$(WIKI_PARSED_FILE): $(JSON_FILE)
	echo "Tokenize data"
	python src/tokenizer.py --wikipedia-raw-file $(JSON_FILE) --wikipedia-tokenized-file $(WIKI_PARSED_FILE) --dump-size 10000

# Preprocess wikipedia to json
$(JSON_FILE): $(XML_FILE)
	echo "Parse to JSON data"
	python -m gensim.scripts.segment_wiki -i -f $(XML_FILE) -o $(JSON_FILE)

# Get wikipedia
$(XML_FILE):
	echo "Get data"
	mkdir -p $(DATA_DIR_LANG)
	wget -P $(DATA_DIR_LANG) $(WIKIURL)

$(BUILDDIR):
	make BUILDDIR=$(BUILDDIR)
